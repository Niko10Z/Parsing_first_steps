{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7873c62",
   "metadata": {},
   "source": [
    "## Install packeges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1b9c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcfe6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8878a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.9.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123b93ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xml-python in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: attrs in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from xml-python) (23.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xml-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86863f2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f14b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import lzma\n",
    "import zipfile\n",
    "import tempfile\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, IO\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57012a",
   "metadata": {},
   "source": [
    "## ArticleInfo class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a93c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleInfo:\n",
    "    header: str\n",
    "    content: str\n",
    "    publication_dt: datetime\n",
    "    parsing_dt: datetime\n",
    "    html: str\n",
    "    href: str\n",
    "    meta_keywords: List[str]\n",
    "    language: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf0654",
   "metadata": {},
   "source": [
    "## Make GET request and return text from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71ad7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(href: str) -> str:\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    cookies = {}\n",
    "    r = requests.get(href, headers=headers, cookies=cookies)\n",
    "    if r.status_code != 200:\n",
    "        with open('pars_log.txt', 'a') as f:\n",
    "            f.write(f\"\"\"Error getting info from URL.\n",
    "                    URL:{href}\n",
    "                    Status:{r.status_code}\n",
    "                    Reason:{r.reason}\n",
    "                    Time:{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\"\")\n",
    "        raise requests.exceptions.RequestException(response=r)\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14798478",
   "metadata": {},
   "source": [
    "## Parse page with short news by tag and page number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6e04816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_coindeskcom_news_page(news_tag: str, num_page: int) -> List:\n",
    "    try:\n",
    "        page_html = get_article_info(f'https://www.coindesk.com/tag/{news_tag}/{num_page}')\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(err)\n",
    "        return None\n",
    "    soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "    short_news = soup.find_all('div', class_='articleTextSection')\n",
    "    news_list = []\n",
    "    for item in short_news:\n",
    "        title = item.find('a', class_='card-title')\n",
    "        pub_date = item.find('div',\n",
    "                              class_='timing-data').find('span',\n",
    "                                                         class_='typography__StyledTypography-owin6q-0 fUOSEs').text\n",
    "        tmp_news = {\n",
    "            'category': item.find('a', class_='category').text,\n",
    "            'title': title.text,\n",
    "            'link': 'https://coindesk.com' + title.attrs['href'],\n",
    "            'description': item.find('span', class_='content-text').text,\n",
    "            'author': item.find('a', class_='ac-author').text,\n",
    "            'pub_datetime': datetime.strptime(pub_date.replace('.', ''), '%b %d, %Y at %I:%M %p %Z')\n",
    "        }\n",
    "        news_list.append(tmp_news)\n",
    "\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45b1c2",
   "metadata": {},
   "source": [
    "## Get news list from RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "486435bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coindeskcom_articles_from_rss(from_dt: datetime=datetime.now(), to_dt: datetime=None) -> List:\n",
    "    xml_data = get_article_info('https://www.coindesk.com/arc/outboundfeeds/rss/')\n",
    "    if xml_data == '':\n",
    "        with open('pars_log.txt', 'a') as f:\n",
    "            f.write(f\"\"\"Error getting RSS.\n",
    "                        URL: https://www.coindesk.com/arc/outboundfeeds/rss/\n",
    "                        Reason: Empty result\n",
    "                        Time:{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\"\")\n",
    "    soup = BeautifulSoup(xml_data, 'xml')\n",
    "    dl = []\n",
    "    all_items = soup.find_all('item')\n",
    "    for item in all_items:\n",
    "        try:\n",
    "            row = {\n",
    "                'category': item.find('category').text,\n",
    "                'guid': item.find('guid').text,\n",
    "                'title': item.find('title').text,\n",
    "                'pub_datetime': datetime.strptime(item.find('pubDate').text, \"%a, %d %b %Y %H:%M:%S %z\"),\n",
    "                'description': item.find('description').text,\n",
    "                'link': item.find('link').text\n",
    "            }\n",
    "            if to_dt:\n",
    "                row['pub_datetime'] = datetime.strptime(row['pub_datetime'].strftime('%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%M:%S')\n",
    "                if to_dt <= row['pub_datetime'] <= from_dt:\n",
    "                    dl.append(row)\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            with open('pars_log.txt', 'a') as f:\n",
    "                f.write(f\"\"\"coindesk.com RSS parsing error.\n",
    "                        Item: {item.__dict__}\n",
    "                        Error: {err}\n",
    "                        Time:{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\"\")\n",
    "            continue\n",
    "\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d813a4de",
   "metadata": {},
   "source": [
    "## Get all news in time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eae762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_articles_coindeskcom(from_dt: datetime, to_dt: datetime) -> List[dict]:\n",
    "    web3_tags = [\n",
    "        'yuga-labs',\n",
    "        'nfts',\n",
    "        'metaverse',\n",
    "        'dao',\n",
    "        'gaming'\n",
    "    ]\n",
    "    articles_list = []\n",
    "    if not from_dt:\n",
    "        from_dt = datetime.now()\n",
    "    if not to_dt:\n",
    "        from_dt = datetime(1970, 1, 1, 0, 0, 0)\n",
    "    for w3_tag in web3_tags:\n",
    "        print(f'Get {w3_tag} news')\n",
    "        page_num = 0\n",
    "        while True:\n",
    "            page_num += 1\n",
    "            news_page_articles = parse_coindeskcom_news_page(w3_tag, page_num)\n",
    "            if to_dt <= news_page_articles[-1]['pub_datetime'] and news_page_articles[0]['pub_datetime'] <= from_dt:\n",
    "                articles_list.extend(news_page_articles)\n",
    "            else:\n",
    "                while news_page_articles and news_page_articles[0]['pub_datetime'] > from_dt:\n",
    "                    news_page_articles.pop(0)\n",
    "                while news_page_articles and news_page_articles[-1]['pub_datetime'] < to_dt:\n",
    "                    news_page_articles.pop()\n",
    "                if news_page_articles:\n",
    "                    articles_list.extend(news_page_articles)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    return articles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d871a3",
   "metadata": {},
   "source": [
    "## Parse one article and return ArticleInfo object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8edaac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_coindeskcom(href: str) -> ArticleInfo:\n",
    "    html_info = get_article_info(href)\n",
    "    if html_info == '':\n",
    "        with open('pars_log.txt', 'a') as f:\n",
    "            f.write(f\"\"\"coindesk.com article parsing error.\n",
    "                    URL: {href}\n",
    "                    Reason: Empty result\n",
    "                    Time:{datetime.now().strftime('%Y-%m-%d %H:%M:%S %Z')}\\n\"\"\")\n",
    "        return None\n",
    "    soup = BeautifulSoup(html_info, \"html.parser\")\n",
    "    ainfo = ArticleInfo()\n",
    "    content_classes = [\n",
    "        'common-textstyles__StyledWrapper-sc-18pd49k-0 eSbCkN',\n",
    "        'headingstyles__StyledWrapper-l955mv-0 fMEozb',\n",
    "        'liststyles__StyledWrapper-sc-13iatdm-0 eksenZ'\n",
    "    ]\n",
    "\n",
    "    def publication_dt_check(tag):\n",
    "        return (tag.name == 'div' and\n",
    "                tag.has_attr('class') and\n",
    "                'at-created' in tag.attrs['class']) or \\\n",
    "               (tag.name == 'span' and\n",
    "                tag.has_attr('class') and\n",
    "                'typography__StyledTypography-owin6q-0' in tag.attrs['class'] and\n",
    "                'fUOSEs' in tag.attrs['class'])\n",
    "    try:\n",
    "        ainfo.header = soup.find('div', class_='at-headline').text\n",
    "        ainfo.content = soup.find('div', class_='at-subheadline').text\n",
    "        ainfo.content += '\\n'.join(i.text for i in soup.find('div', class_='at-content-wrapper').find_all('div', class_=content_classes))\n",
    "        if soup.find('div', class_='at-category').text == 'Opinion':\n",
    "            ainfo.publication_dt = datetime.strptime(soup.find(publication_dt_check).text.replace('.', ''),\n",
    "                                                     \"%b %d, %Y at %I:%M %p %Z\")  # %r\n",
    "        else:\n",
    "            ainfo.publication_dt = datetime.strptime(soup.find('div', class_='at-created').text.replace('.', ''),\n",
    "                                                    \"%b %d, %Y at %I:%M %p %Z\") #%r\n",
    "        ainfo.parsing_dt = datetime.now()\n",
    "        ainfo.language = soup.find('div', class_='footer-selectstyles__StyledRootContainer-sxto8j-0 lkWIzk').text\n",
    "        ainfo.html = html_info\n",
    "        ainfo.href = href\n",
    "    except Exception as err:\n",
    "        with open('pars_log.txt', 'a') as f:\n",
    "            f.write(f\"\"\"cointelegraph.com article parsing error.\n",
    "                    URL: {href}\n",
    "                    Error: {err}\n",
    "                    Time:{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\"\")\n",
    "        return None\n",
    "\n",
    "    return ainfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd3819",
   "metadata": {},
   "source": [
    "## Save archive with html and json to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a476fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_disk(file_name: str = 'article.xz', article: ArticleInfo = None) -> None:\n",
    "    print(f'Try to save {file_name} file')\n",
    "    if not article:\n",
    "        with open('pars_log.txt', 'a') as f:\n",
    "            f.write(f\"\"\"Article save error.\n",
    "                    file_name: {file_name}\n",
    "                    Reason: Empty article\n",
    "                    Time:{datetime.now().strftime('%Y-%m-%d %H:%M:%S %Z')}\\n\"\"\")\n",
    "        return\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        with open(os.path.join(temp_dir, 'article.html'), 'wb') as f:\n",
    "            f.write(lzma.compress(bytes(article.html, 'utf-8')))\n",
    "        json_obj = article.__dict__\n",
    "        json_obj['publication_dt'] = json_obj['publication_dt'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        json_obj['parsing_dt'] = json_obj['parsing_dt'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        with open(os.path.join(temp_dir, 'article.json'), \"wb\") as f:\n",
    "            f.write(lzma.compress(bytes(json.dumps(json_obj, indent=4), 'utf-8')))\n",
    "        with zipfile.ZipFile(file_name, \"w\") as zpf:\n",
    "            zpf.write(os.path.join(temp_dir, 'article.html'), 'article.html')\n",
    "            zpf.write(os.path.join(temp_dir, 'article.json'), 'article.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98124b",
   "metadata": {},
   "source": [
    "## Get last parsing datetime from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db7b93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_pars_dt() -> datetime:\n",
    "    try:\n",
    "        with open('last_parsing.txt','r') as dtf:\n",
    "            return datetime.strptime(dtf.read(), '%Y-%m-%d %H:%M:%S')\n",
    "    except FileNotFoundError:\n",
    "        with open('pars_log.txt', 'a') as f:\n",
    "            f.write(f\"\"\"Time get error.\n",
    "                    Reason: No datetime file\n",
    "                    Time:{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\"\")\n",
    "            return datetime(1970, 1, 1, 0, 0, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e6743",
   "metadata": {},
   "source": [
    "## Decompress archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4716912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_archive(file_name):\n",
    "    with zipfile.ZipFile(file_name, \"r\") as fp:\n",
    "        with open('article.html', 'wb') as html_file, open('article.json', 'wb') as json_file:\n",
    "            html_file.write(lzma.decompress(fp.read('article.html')))\n",
    "            json_file.write(lzma.decompress(fp.read('article.json')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1898a",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0598bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get yuga-labs news\n",
      "Get nfts news\n",
      "Get metaverse news\n",
      "Get dao news\n",
      "Get gaming news\n",
      "Try to save Web3PixelPenguinsanNFTCharityScamShowsDangersOfNFTInfluencerCulture.xz file\n",
      "Try to save Web3NFTArtistFewociousRevealsUpcomingCollectionFewos.xz file\n",
      "Try to save Web3SothebysSecond3ACNFTAuctionIncludesLandmarkDmitriCherniakWork.xz file\n",
      "Try to save Web3FindSatoshiLabsRollsOutAIToolThatTurnsSelfiesIntoNFTs.xz file\n",
      "Try to save Web3BRC721ETokenStandardConvertsEthereumNFTstoBitcoinNFTs.xz file\n",
      "Try to save Web3MercedesBenzWeb3ArmToReleaseNFTCollectionWithDigitalArtCommunityFingerprintsDAO.xz file\n",
      "Try to save Web3NikeTripsUpSWOOSHLaunchWhileBitcoinNFTsSoar.xz file\n",
      "Try to save Web3NikeOF1NFTSaleSurpasses1MDespiteDelaysTechIssues.xz file\n",
      "Try to save Web3PudgyPenguinsNFTProjectOnceEndangeredProvesWeb3TurnaroundIsPossible.xz file\n",
      "Try to save Web3BinanceLaunchingNFTLoanFeature.xz file\n",
      "Try to save Web3BitcoinBasedSpacePepesLedWeeklyTradingVolumesAmongNFTCollections.xz file\n",
      "Try to save Web3Web3FriendlyBrowserBraveIntroducesNFTGatedVideoCalls.xz file\n",
      "Try to save Web3AvaLabsLaunchesNoCodeWeb3LaunchpadAvaCloud.xz file\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    news_list = get_all_articles_coindeskcom(datetime(2023, 5, 31, 23, 59), datetime(2023, 5, 24, 0, 0))\n",
    "#     last_pars_time = get_last_pars_dt()\n",
    "#     news_list = get_coindeskcom_articles_from_rss(from_dt=datetime.now(), to_dt=last_pars_time)\n",
    "    for item in news_list:\n",
    "        tmp_article = parse_article_coindeskcom(item['link'])\n",
    "        if not tmp_article:\n",
    "            continue\n",
    "        try:\n",
    "            save_to_disk(re.sub(r'[^a-zA-z0-9]', '', f'{item[\"category\"]}{item[\"title\"]}')+'.xz', tmp_article)\n",
    "        except Exception as err:\n",
    "            with open('pars_log.txt', 'a') as f:\n",
    "                f.write(f\"\"\"Article save error.\n",
    "                        Item: {item}\n",
    "                        Error: {err}\n",
    "                        Time:{datetime.now().strftime('%Y-%m-%d %H:%M:%S %Z')}\\n\"\"\")\n",
    "\n",
    "    with open('last_parsing.txt', 'w') as f:\n",
    "        f.write(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b6f0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

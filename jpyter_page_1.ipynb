{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b551e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72fa11e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b790d473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.9.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d6f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xml-python in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: attrs in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from xml-python) (23.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xml-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14869a9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58cfbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import lzma\n",
    "import zipfile\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, IO\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd56c3",
   "metadata": {},
   "source": [
    "## Make GET request and return text from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d56730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_url(href: str) -> str:\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    cookies = {}\n",
    "    r = requests.get(href, headers=headers, cookies=cookies)\n",
    "    print(href)\n",
    "    print(r.status_code)\n",
    "    print(r.reason)\n",
    "    # print(r.headers)\n",
    "    # print(r.cookies)\n",
    "    # print(r.content)\n",
    "    # print(r.text)\n",
    "    # print(dir(r))\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f1bcd7",
   "metadata": {},
   "source": [
    "## Parse RSS from coindesk.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0f279",
   "metadata": {},
   "source": [
    "get str with XML-data, \n",
    "parse it \n",
    "and return list of dicts with base info about news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf936f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_coindeskcom_rss(xml_data: str) -> List:\n",
    "    soup = BeautifulSoup(xml_data, 'xml')\n",
    "    dl = []\n",
    "    all_items = soup.find_all('item')\n",
    "    for item in all_items:\n",
    "        row = {\n",
    "            'guid': item.find('guid').text,\n",
    "            'title': item.find('title').text,\n",
    "            'pubDate': datetime.strptime(item.find('pubDate').text, \"%a, %d %B %Y %H:%M:%S %z\"),\n",
    "            'description': item.find('description').text,\n",
    "            'link': item.find('link').text\n",
    "        }\n",
    "\n",
    "        dl.append(row)\n",
    "\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee9ccc",
   "metadata": {},
   "source": [
    "## Parse RSS from cointelegraphcom.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53257821",
   "metadata": {},
   "source": [
    "get str with XML-data, \n",
    "parse it \n",
    "and return list of dicts with base info about news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef3c790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cointelegraphcom_rss(xml_data: str) -> List:\n",
    "    soup = BeautifulSoup(xml_data, 'xml')\n",
    "    dl = []\n",
    "    all_items = soup.find_all('item')\n",
    "    for item in all_items:\n",
    "        if item.find('link').text.find('https://cointelegraph.com/news'):\n",
    "            continue\n",
    "\n",
    "        guid = item.find('guid').text\n",
    "        row = {\n",
    "            'guid': guid[guid.rfind(\"/\")+1:],\n",
    "            'title': item.find('title').text,\n",
    "            'pubDate': datetime.strptime(item.find('pubDate').text, \"%a, %d %B %Y %H:%M:%S %z\"),\n",
    "            'description': item.find('description').text,\n",
    "            'link': item.find('link').text\n",
    "        }\n",
    "        dl.append(row)\n",
    "\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230b739",
   "metadata": {},
   "source": [
    "## class ArticleInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56689c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleInfo:\n",
    "    header: str\n",
    "    content: str\n",
    "    publication_dt: datetime\n",
    "    parsing_dt: datetime\n",
    "    html: str\n",
    "    href: str\n",
    "    meta_keywords: List[str]\n",
    "    language: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d067a",
   "metadata": {},
   "source": [
    "## Parse one article from cointelegraph.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66dc6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_cointelegraphcom(href: str) -> ArticleInfo:\n",
    "    html_info = get_text_from_url(href)\n",
    "    soup = BeautifulSoup(html_info, \"html.parser\")\n",
    "    ainfo = ArticleInfo()\n",
    "    # ainfo.language = soup.find('button', {'data-testid': 'language-button'}).text\n",
    "    ainfo.language = 'English'\n",
    "    soup = soup.find('div', class_='post-page__item')\n",
    "    ainfo.header = soup.find('h1', class_='post__title').text\n",
    "    ainfo.content = soup.find('p', class_='post__lead').text\n",
    "    ainfo.content += '\\n'.join(i.text for i in soup.find('div', class_='post__content-wrapper').find_all('p'))\n",
    "    ainfo.publication_dt = datetime.strptime(soup.find('time').attrs['datetime'], \"%Y-%m-%d\")  # %r\n",
    "    ainfo.parsing_dt = datetime.now()\n",
    "    ainfo.html = html_info\n",
    "    ainfo.href = href\n",
    "\n",
    "    return ainfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ded17f",
   "metadata": {},
   "source": [
    "## Parse one article from coindesk.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efe86192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_coindeskcom(href: str) -> ArticleInfo:\n",
    "    html_info = get_text_from_url(href)\n",
    "    soup = BeautifulSoup(html_info, \"html.parser\")\n",
    "    ainfo = ArticleInfo()\n",
    "    content_classes = [\n",
    "        'common-textstyles__StyledWrapper-sc-18pd49k-0 eSbCkN',\n",
    "        'headingstyles__StyledWrapper-l955mv-0 fMEozb',\n",
    "        'liststyles__StyledWrapper-sc-13iatdm-0 eksenZ'\n",
    "    ]\n",
    "\n",
    "    def publication_dt_check(tag):\n",
    "        return (tag.name == 'div' and\n",
    "                tag.has_attr('class') and\n",
    "                'at-created' in tag.attrs['class']) or \\\n",
    "               (tag.name == 'span' and\n",
    "                tag.has_attr('class') and\n",
    "                'typography__StyledTypography-owin6q-0' in tag.attrs['class'] and\n",
    "                'fUOSEs' in tag.attrs['class'])\n",
    "\n",
    "    ainfo.header = soup.find('div', class_='at-headline').text\n",
    "    ainfo.content = soup.find('div', class_='at-subheadline').text\n",
    "    ainfo.content += '\\n'.join(i.text for i in soup.find('div', class_='at-content-wrapper').find_all('div', class_=content_classes))\n",
    "    if soup.find('div', class_='at-category').text == 'Opinion':\n",
    "        ainfo.publication_dt = datetime.strptime(soup.find(publication_dt_check).text.replace('.', ''),\n",
    "                                                 \"%B %d, %Y at %I:%M %p %Z\")  # %r\n",
    "    else:\n",
    "        ainfo.publication_dt = datetime.strptime(soup.find('div', class_='at-created').text.replace('.', ''),\n",
    "                                                \"%B %d, %Y at %I:%M %p %Z\") #%r\n",
    "    ainfo.parsing_dt = datetime.now()\n",
    "    ainfo.language = soup.find('div', class_='footer-selectstyles__StyledRootContainer-sxto8j-0 lkWIzk').text\n",
    "    ainfo.html = html_info\n",
    "    ainfo.href = href\n",
    "\n",
    "    return ainfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522cb6e",
   "metadata": {},
   "source": [
    "## save_to_disk function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30453f6",
   "metadata": {},
   "source": [
    "get file_name and ArticleInfo object\n",
    "create files article.html and article.json\n",
    "compress it\n",
    "save it into archive with file_name\n",
    "and delete from disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "677dad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_disk(file_name: str = 'article.xz', article: ArticleInfo = None) -> None:\n",
    "    if not article:\n",
    "        print('Nothing to save')\n",
    "        return\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        with open(os.path.join(temp_dir, 'article.html'), 'wb') as f:\n",
    "            f.write(lzma.compress(bytes(article.html, 'utf-8')))\n",
    "        json_obj = article.__dict__\n",
    "        json_obj['publication_dt'] = json_obj['publication_dt'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        json_obj['parsing_dt'] = json_obj['parsing_dt'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        with open(os.path.join(temp_dir, 'article.json'), \"wb\") as f:\n",
    "            f.write(lzma.compress(bytes(json.dumps(json_obj, indent=4), 'utf-8')))\n",
    "        with zipfile.ZipFile(file_name, \"w\") as zpf:\n",
    "            zpf.write(os.path.join(temp_dir, 'article.html'), 'article.html')\n",
    "            zpf.write(os.path.join(temp_dir, 'article.json'), 'article.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c808d54",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa96f251",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # xml_info = get_text_from_url('https://www.coindesk.com/arc/outboundfeeds/rss/')\n",
    "    # news_list = parse_coindeskcom_rss(xml_info)\n",
    "    # for item in news_list:\n",
    "    #     tmp_article = parse_article_coindeskcom(item['link'])\n",
    "    #     save_to_disk(f'{item[\"guid\"]}.xz', tmp_article)\n",
    "\n",
    "#     xml_info = get_text_from_url('https://cointelegraph.com/rss')\n",
    "#     news_list = parse_cointelegraphcom_rss(xml_info)\n",
    "#     for item in news_list:\n",
    "#         tmp_article = parse_article_cointelegraphcom(item['link'])\n",
    "#         save_to_disk(f'{item[\"guid\"]}.xz', tmp_article)\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

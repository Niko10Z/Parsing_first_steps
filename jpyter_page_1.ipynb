{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b551e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72fa11e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nikoloz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b790d473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Using cached lxml-4.9.2-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-4.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14869a9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58cfbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import lzma\n",
    "import zipfile\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from typing import List, IO\n",
    "from xml.dom.minidom import parse, parseString, Document\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd56c3",
   "metadata": {},
   "source": [
    "## Make GET request and return text from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d56730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_url(href: str) -> str:\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    cookies = {}\n",
    "    r = requests.get(href, headers=headers, cookies=cookies)\n",
    "    print(href)\n",
    "    print(r.status_code)\n",
    "    print(r.reason)\n",
    "    # print(r.headers)\n",
    "    # print(r.cookies)\n",
    "    # print(r.content)\n",
    "    # print(r.text)\n",
    "    # print(dir(r))\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f1bcd7",
   "metadata": {},
   "source": [
    "## Parse RSS from coindesk.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0f279",
   "metadata": {},
   "source": [
    "get str with XML-data, \n",
    "parse it \n",
    "and return list of dicts with base info about news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cf936f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_coindeskcom_rss(xml_data: str) -> List:\n",
    "    soup = BeautifulSoup(xml_data, 'xml')\n",
    "    dl = []\n",
    "    all_items = soup.find_all('item')\n",
    "    for item in all_items:\n",
    "        row = {\n",
    "            'guid': item.find('guid').text,\n",
    "            'title': item.find('title').text,\n",
    "            'pubDate': datetime.strptime(item.find('pubDate').text, \"%a, %d %B %Y %H:%M:%S %z\"),\n",
    "            'description': item.find('description').text,\n",
    "            'link': item.find('link').text\n",
    "        }\n",
    "\n",
    "        dl.append(row)\n",
    "\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee9ccc",
   "metadata": {},
   "source": [
    "## Parse RSS from cointelegraphcom.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53257821",
   "metadata": {},
   "source": [
    "get str with XML-data, \n",
    "parse it \n",
    "and return list of dicts with base info about news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef3c790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cointelegraphcom_rss(xml_data: str) -> List:\n",
    "    soup = BeautifulSoup(xml_data, 'xml')\n",
    "    dl = []\n",
    "    all_items = soup.find_all('item')\n",
    "    for item in all_items:\n",
    "        if item.find('link').text.find('https://cointelegraph.com/news'):\n",
    "            continue\n",
    "\n",
    "        guid = item.find('guid').text\n",
    "        row = {\n",
    "            'guid': guid[guid.rfind(\"/\")+1:],\n",
    "            'title': item.find('title').text,\n",
    "            'pubDate': datetime.strptime(item.find('pubDate').text, \"%a, %d %B %Y %H:%M:%S %z\"),\n",
    "            'description': item.find('description').text,\n",
    "            'link': item.find('link').text\n",
    "        }\n",
    "        dl.append(row)\n",
    "\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230b739",
   "metadata": {},
   "source": [
    "## class ArticleInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56689c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleInfo:\n",
    "    header: str\n",
    "    content: str\n",
    "    publication_dt: datetime\n",
    "    parsing_dt: datetime\n",
    "    html: str\n",
    "    href: str\n",
    "    meta_keywords: List[str]\n",
    "    language: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d067a",
   "metadata": {},
   "source": [
    "## Parse one article from cointelegraph.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66dc6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_cointelegraphcom(href: str) -> ArticleInfo:\n",
    "    html_info = get_text_from_url(href)\n",
    "    soup = BeautifulSoup(html_info, \"html.parser\")\n",
    "    ainfo = ArticleInfo()\n",
    "    # ainfo.language = soup.find('button', {'data-testid': 'language-button'}).text\n",
    "    ainfo.language = 'English'\n",
    "    soup = soup.find('div', class_='post-page__item')\n",
    "    ainfo.header = soup.find('h1', class_='post__title').text\n",
    "    ainfo.content = soup.find('p', class_='post__lead').text\n",
    "    ainfo.content += '\\n'.join(i.text for i in soup.find('div', class_='post__content-wrapper').find_all('p'))\n",
    "    ainfo.publication_dt = datetime.strptime(soup.find('time').attrs['datetime'], \"%Y-%m-%d\")  # %r\n",
    "    ainfo.parsing_dt = datetime.now()\n",
    "    ainfo.html = html_info\n",
    "    ainfo.href = href\n",
    "\n",
    "    return ainfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ded17f",
   "metadata": {},
   "source": [
    "## Parse one article from coindesk.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efe86192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_coindeskcom(href: str) -> ArticleInfo:\n",
    "    html_info = get_text_from_url(href)\n",
    "    soup = BeautifulSoup(html_info, \"html.parser\")\n",
    "    ainfo = ArticleInfo()\n",
    "    content_classes = [\n",
    "        'common-textstyles__StyledWrapper-sc-18pd49k-0 eSbCkN',\n",
    "        'headingstyles__StyledWrapper-l955mv-0 fMEozb',\n",
    "        'liststyles__StyledWrapper-sc-13iatdm-0 eksenZ'\n",
    "    ]\n",
    "\n",
    "    def publication_dt_check(tag):\n",
    "        return (tag.name == 'div' and\n",
    "                tag.has_attr('class') and\n",
    "                'at-created' in tag.attrs['class']) or \\\n",
    "               (tag.name == 'span' and\n",
    "                tag.has_attr('class') and\n",
    "                'typography__StyledTypography-owin6q-0' in tag.attrs['class'] and\n",
    "                'fUOSEs' in tag.attrs['class'])\n",
    "\n",
    "    ainfo.header = soup.find('div', class_='at-headline').text\n",
    "    ainfo.content = soup.find('div', class_='at-subheadline').text\n",
    "    ainfo.content += '\\n'.join(i.text for i in soup.find('div', class_='at-content-wrapper').find_all('div', class_=content_classes))\n",
    "    if soup.find('div', class_='at-category').text == 'Opinion':\n",
    "        ainfo.publication_dt = datetime.strptime(soup.find(publication_dt_check).text.replace('.', ''),\n",
    "                                                 \"%B %d, %Y at %I:%M %p %Z\")  # %r\n",
    "    else:\n",
    "        ainfo.publication_dt = datetime.strptime(soup.find('div', class_='at-created').text.replace('.', ''),\n",
    "                                                \"%B %d, %Y at %I:%M %p %Z\") #%r\n",
    "    ainfo.parsing_dt = datetime.now()\n",
    "    ainfo.language = soup.find('div', class_='footer-selectstyles__StyledRootContainer-sxto8j-0 lkWIzk').text\n",
    "    ainfo.html = html_info\n",
    "    ainfo.href = href\n",
    "\n",
    "    return ainfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522cb6e",
   "metadata": {},
   "source": [
    "## save_to_disk function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30453f6",
   "metadata": {},
   "source": [
    "get file_name and ArticleInfo object\n",
    "create files article.html and article.json\n",
    "compress it\n",
    "save it into archive with file_name\n",
    "and delete from disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "677dad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_disk(file_name: str = 'article.xz', article: ArticleInfo = None) -> None:\n",
    "    if not article:\n",
    "        print('Nothing to save')\n",
    "        return\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        with open(os.path.join(temp_dir, 'article.html'), 'wb') as f:\n",
    "            f.write(lzma.compress(bytes(article.html, 'utf-8')))\n",
    "        json_obj = article.__dict__\n",
    "        json_obj['publication_dt'] = json_obj['publication_dt'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        json_obj['parsing_dt'] = json_obj['parsing_dt'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        with open(os.path.join(temp_dir, 'article.json'), \"wb\") as f:\n",
    "            f.write(lzma.compress(bytes(json.dumps(json_obj, indent=4), 'utf-8')))\n",
    "        with zipfile.ZipFile(file_name, \"w\") as zpf:\n",
    "            zpf.write(os.path.join(temp_dir, 'article.html'), 'article.html')\n",
    "            zpf.write(os.path.join(temp_dir, 'article.json'), 'article.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c808d54",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa96f251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cointelegraph.com/rss\n",
      "200\n",
      "OK\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: xml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# xml_info = get_text_from_url('https://www.coindesk.com/arc/outboundfeeds/rss/')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# news_list = parse_coindeskcom_rss(xml_info)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# for item in news_list:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#     tmp_article = parse_article_coindeskcom(item['link'])\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#     save_to_disk(f'{item[\"guid\"]}.xz', tmp_article)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     xml_info \u001b[38;5;241m=\u001b[39m get_text_from_url(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://cointelegraph.com/rss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     news_list \u001b[38;5;241m=\u001b[39m \u001b[43mparse_cointelegraphcom_rss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m news_list:\n\u001b[0;32m     11\u001b[0m         tmp_article \u001b[38;5;241m=\u001b[39m parse_article_cointelegraphcom(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m, in \u001b[0;36mparse_cointelegraphcom_rss\u001b[1;34m(xml_data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_cointelegraphcom_rss\u001b[39m(xml_data: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m----> 2\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     dl \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m     all_items \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: xml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # xml_info = get_text_from_url('https://www.coindesk.com/arc/outboundfeeds/rss/')\n",
    "    # news_list = parse_coindeskcom_rss(xml_info)\n",
    "    # for item in news_list:\n",
    "    #     tmp_article = parse_article_coindeskcom(item['link'])\n",
    "    #     save_to_disk(f'{item[\"guid\"]}.xz', tmp_article)\n",
    "\n",
    "    xml_info = get_text_from_url('https://cointelegraph.com/rss')\n",
    "    news_list = parse_cointelegraphcom_rss(xml_info)\n",
    "    for item in news_list:\n",
    "        tmp_article = parse_article_cointelegraphcom(item['link'])\n",
    "        save_to_disk(f'{item[\"guid\"]}.xz', tmp_article)\n",
    "        # save_json_to_disk(f'{item[\"guid\"]}.json', tmp_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379a37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
